---
title:    "Bring Your Own Data (BYOD) Series"
subtitle: "Summarizing and Visualizing Your Data in R"
author:   "MN-DNR Biometricians"
date:     today
engine:   knitr
format: 
  html: 
    highlight: tango
    css: camp_style.css
    number-sections: true
    fontsize: 14pt
    toc: true
    mainfont: Calibri
    monofont: Aptos Mono
project:
  type: website
  output-dir: docs
---

```{r fonts}
#| echo: false
htmltools::tagList(rmarkdown::html_dependency_font_awesome())
```

```{css your-turn-callout}
#| echo: false
{
}
.YourTurn {
  margin:        0em;
  border:        2px solid #003865;
  border-left:   5px solid #003865; 
  border-radius: 10px;
}
.YourTurn-header {
  margin-left:   0em;
  margin-top:    0em;
  margin-bottom: 0em;
   /* align-items: center; */
   /* vertical-align: text-bottom; */
  padding-left:   10px;
  padding-top:    5px;
  padding-bottom: 0px;
  background-color: #0080A6;
  border: 2px solid #003865;
  border-left: 10px solid #003865; 
  border-bottom: 1px solid #003865; 
  border-top-right-radius: 10px;
  color: white;
  font-size: 1.3em;
  font-weight: bold;
  /* background-size: 25px;
  background-repeat: no-repeat;
  background-position: 10px center;
  background-image: url("fa-code-icon.png"); */
}
.YourTurn-container {
  border: 2px solid #003865;
  border-left: 10px solid #003865; 
  border-top: 1px solid #003865; 
  padding-top: 5px;
  padding-left: 10px;
  padding-right: 10px;
  padding-bottom: 5px;
  color: black;
  background-color: white;
  border-bottom-right-radius: 10px;
}
```

# Workshop goals

The goal of this workshop is to provide the tools and hands-on experience necessary so that participants are comfortable importing and tidying up their data in R, creating summaries, and generating publication-quality figures. If time allows we could also explore simple statistical models during the practice section of the workshop.

Throughout this workshop we will use the syntax of the `tidyverse` group of packages, which offers an efficient way of coding and building complexity in data manipulation steps in an organized and structured way.  

The content presented in this document was inspired by some of the courses presented at [MDH R Camp](https://tidy-mn.github.io/R-camp-penguins/). We encourage all participants to take a look at the material generously shared by MDH by following that link; it contains excellent instructional videos and coding examples.

# Setting-up your work space

Let's set up our work space for the day by first creating a new RStudio project for this workshop. All of the scripts and data used in the workshop should be saved to that project so that everything can be easily referenced in the future.

We will now take a few moments to demonstrate how to set-up a new RStudio project. When that is done, we'll create folders for data, scripts, and outputs in our project folder. Finally, let's create a new R script. We can name it penguins_data_demo.r or you can chose another script name if you'd like. This script will contain everything that we will demonstrate in this class with a dataset that we will provide (penguins!).

# Loading your tools

## Packages

In this section, we will learn about the different packages that we will be using in this workshop and will explore different functions for reading files into r.

```{r packages-info}
#| eval: true
#| warning: false
#| echo: false

# load table package
library(kableExtra)

# create dataframe of package info
packages_info <- data.frame(
  Package = c("readr", "readxl", "tidyr", "dplyr", "stringr", "janitor", "ggplot2", "tidyverse"),
  Purpose = c("Read delimited files","Read excel files", 
             "Tidy messy data", 
             "Format and summarize data tables",
             "Format character strings",
             "More data cleaning (not in the tidyverse)",
             "Create elegant data visualisations using the grammar of graphics",
             "Load many of the packages in the tidyverse")
)

# visualize table
kable(packages_info, caption = "Essential R packages for tidying and visualizing data") |>
  kable_styling(bootstrap_options = c("striped", "hover"))

```

Let's load packages!

```{r load-packages}
#| eval: true
#| warning: false
#| output: false

# load packages in the tidyverse (e.g., readr, readxl, tidyr)
library(tidyverse) 
library(readxl)

# we can check what packages were loaded
names(sessionInfo()[["otherPkgs"]])
```

## Loading data

Let's load a few datasets in different formats. First, you'll need to download the data to your computer and save it in a folder called "data" that's inside of your RStudio project.

[penguin_data_final.xlsx](https://github.com/MN-DNR-R/BYOD_Seminar1_Data_Summaries_and_Visualization/raw/refs/heads/main/data/penguin_data_final.xlsx)

Since we stored the data inside a folder in the project, the file path is very simple. If you need to load data that is stored outside the project, you will need to specify the whole file path.

```{r load-excel}
#| eval: false

# load a .xlsx file into your R environment
mydat_xls <- read_excel("data/penguin_data_final.xlsx") 

```

Next, let's try loading data from a url. We would use the read_csv function for data on our computer as well.

```{r load-url}
#| eval: false

# load a .csv file into your R environment
mydat_csv <- read_csv("https://raw.githubusercontent.com/MN-DNR-R/BYOD_Seminar1_Data_Summaries_and_Visualization/refs/heads/main/data/penguin_data_final.csv?token=GHSAT0AAAAAADGVPTNKVJGII66G3RODGYUA2JGFBOA") 

```

A summary of the data is generated when you use the "read_" functions. You can also look in the environment and make sure that the files were loaded properly! We will also demonstrate ways to explore your data in the next section.

## Data from a package

Some datasets are stored inside of R packages. We will use one that's in a package for the rest of our demonstration.

The `palmerpenguins` package was created by Allison Horst, Alison Hill, and Kristen Gorman as a tool for teaching data manipulation and analysis techniques in R. In the following sections we will explore the data available within this package, and we will learn to manipulate and visualize the data using a variety of tools and techniques. Below you will use the `citation()` command line to get the full citation for the package!

The library contains two datasets which can be called without having to read them manually (as we previously did with the .csv or .xlsx files). One which contains raw penguins data (penguin_raw), and the second file (penguins) is the simplified (i.e., clean) dataset. 

You also can read more about the package here: [PalmerPenguins](https://allisonhorst.github.io/palmerpenguins/)

```{r palmer}
#| eval: true
#| warning: false
#| message: false
#| output: false

# install package (delete the '#' to run the line and use the '#' after you've installed the package)
# install.packages("palmerpenguins")

# load library into your R environment 
library(palmerpenguins)

# get information about the package:
?palmerpenguins

# Get proper citation for the package (e.g., to use in a publication). Note that you can use the "citation" function for any of the packages used in an analysis, which is very useful when writing the methods in a report or peer-reviewed manuscript. 
citation("palmerpenguins")

# let's save the raw data in our environment
rawdat <- penguins_raw
```

# Getting to know your data

## Data exploration functions

There are many functions available to explore the data. Here is a list of the main ones that we will be using in this workshop.

```{r useful-functions}
#| echo: false

useful_fct_1 <- data.frame(Function = c("glimpse()", "str()", "names()", "head()", "tail()"," nrow()", "ncol()", "summary()", "unique()"),
                           Purpose = c("Overview of dataframe rows, columns, column names, dataframe dimension, and a glimpse of first values", 
                                       "Succinct summary of all columns of a dataframe; similar to glimpse but within base R",
                                       "Column names",
                                       "Display first n rows of data (default is 6)",
                                       "Displays last n rows of data (default is 6)",
                                       "Number of rows",
                                       "Number of columns",
                                       "Summary of the data in each column",
                                       "Displays unique values of a variable"))

kable(useful_fct_1, caption = "Useful functions for data exploration") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))

```

## Data types

Before doing any data manipulation in R, it is important to recognize the different data types. When importing your data into R, R will automatically recognize the type of data of each column. At times, however, a specific data type may need to be set or converted from one to another. 

For example, one could use the `as.numeric()` function to convert a column to *numeric*, or `as.character()` to convert to *character*. When doing such data conversions, it is important to note that some elements of a vector may be converted to NAs if R recognizes that it does not correspond to the data type specified.

A good example of this is when a column that should be *numeric* contains characters (e.g., instead of an actual count one had recorded something like "more then 100 animals"). In this case, this would return an "NA" as R does not know what to do with this information! Good data entry and quality checks prior to importing data into R are very important to help prevent data loss.

With a *factor* data type, you can specify the order of variables, such as "low", "medium", "high". This defined order can then make data visualizations and analyses more intuitive.

```{r data-types}
#| echo: false

data_types <- data.frame(Type = c("Numeric", "Integer", "Logical", "Character", "Factor"),
                         Example = c("1.5, 2.6", "1, 2", "True/False", "One, Two", "Control, Treatment"))

kable(data_types, caption = "Data types supported in R") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))

```

## Examining the data

Let's use the functions from the function description table above to examine what is contained in our dataset.

```{r examine-data}
#| eval: false

# the glimpse function provides a good first overview of the data
glimpse(rawdat)

# the str function also provides a quick but complete overview of the structure of the data
str(rawdat)

# quick summary of all of the data
summary(rawdat)

# print column names
names(rawdat)

# look at first and last rows
head(rawdat, n = 10)
tail(rawdat, n = 10)

# number of rows and columns
nrow(rawdat)
ncol(rawdat)

# lists unique values (this can help identify if there are data entry errors)
unique(rawdat$Species)
unique(rawdat$Island)
unique(rawdat$Region)

# How many unique ID's do we have? How many data points do we have? Do they match?
unique(rawdat$`Individual ID`) 

```

## Data quality check

Looking at the raw data, what could be improved for analyses and data visualizations? Is the dataset pretty clean and analysis-ready, or are these things that need to be addressed and changed so that we don't run into issues later in the process?

Here are some examples:

1.  names in the Species column are long
2.  the Sex and Stage columns are characters, but they would be more useful as factors  

**Exercise:** Can you think of any others?  

# Tidying your data

## What is tidy data?

We refer to "tidy data" as a dataset where each column is a variable, each row is an observation, and each cell contains a single value and a data type that is consistent across a given column (e.g., all numeric). The `tidyverse` packages contains functions for manipulating the data so that we can create a clean, tidy dataset to be used for all analyses.

**Note**: Recall that we will never overwrite the original dataset. The raw data should always remain in its original location and be unaltered. The beauty of R scripts is that we can save all data cleaning and data manipulation steps while retaining the original data

**Why is it important to tidy our data?**

Taking time to tidy a dataset ensures that the data summaries, analyses, or visualizations we produce represent "true" data values to the best of our knowledge (e.g., no wrongly assigned values, missing data, or data entry errors). It is a the first and most critical step of any data analysis, and though it can be a lengthy process, it is worth the effort.

**Why tidy data in R?**

Using R ensures that the original dataset is kept unaltered, and that all data manipulation steps are documented and reproducible. This is a major advantage of tidying data in R as opposed to using spreadsheet applications such as Excel, where there is often few to no records of changes made to a dataset.

## The pipe operator

R uses many different operators such as <- and = along with typical arithmetic (+, -, `*`, /, ^) and relational (<, >, <=, >=, == ,!=) symbols. The *pipe* operator (`%>%` or `|>`) provides a way to efficiently chain functions together in a single statement. It is similar to saying: let's take this data, *and then* do this with it, *and then* do this, kind of like a data assembly line! So ultimately, every time that you see or use `%>%` or `|>` in a script, you can think of it as being the same as if you were reading or coding *and then...*! It is a very useful tool for tidying data.

The operator `|>` is from base R while `%>%` is from the `tidyverse` package `magrittr`. More background on the origin and use of the pipe operator can be found [here](https://www.datacamp.com/tutorial/pipe-r-tutorial).

Let's look at an example of tidying data with and without the pipe operator.

```{r pipe-example}
#| eval: true

# edit the raw data to filter for samples collected from Torgersen Island and arrange the table by the date the egg was collected

# without the pipe operator
torg1 <- filter(rawdat, Island == "Torgersen")
torg2 <- arrange(torg1, 'Date Egg')

# with the pipe operator
torg <- rawdat %>% 
  filter(Island == "Torgersen") %>% 
  arrange('Date Egg')

```

When using the pipe operator, the input from before the pipe becomes the first argument in the function after the pipe. If you want the input to be the second, third, etc. argument, represent its location with `.`.

## Tidy functions

Now that we've introduced a few basic tidy data concepts and tools, we can start tidying our data. There are many very useful functions to subset data, re-arrange columns, create new columns, etc. Below are the ones you may use more frequently as you learn to tidy your data in R and with the `tidyverse` packages.

```{r tidy-functions}
#| echo: false

useful_fct_2<-data.frame(Function=c("filter()",
                                    "select()",
                                    "arrange()",
                                    "mutate()",
                                    "rename()"),
                         Purpose=c("Keep (or exclude) rows that meet specific criteria",
                                   "Select columns to keep (or to drop)",
                                   "Sort a dataframe based on a column's value (or several columns' values)",
                                   "Add new columns or update existing columns",
                                   "Rename columns"))

kable(useful_fct_2, caption = "Useful functions to create tidy data") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))

```

## Formating column names

Some of the column names are difficult to work with because they have spaces and symbols. Let's fix that.

```{r column-names}
#| eval: true
#| output: false

# look at the column names
head(rawdat)
names(rawdat)

# let's make the following changes:
# lowercase names
# replace spaces with _
# replace / with _
# remove parentheses
rawdat2 <- rawdat %>% 
  rename_with(.fn = str_to_lower) %>% 
  rename_with(.fn = ~str_replace_all(.x, " ", "_")) %>% 
  rename_with(.fn = ~str_replace_all(.x, "/", "_")) %>% 
  rename_with(.fn = ~str_remove_all(.x, "\\(|\\)"))

# look at new names
names(rawdat2)
```

The functions `str_to_lower()` and `str_replace_all()` are part of the `stringr` package, automatically loaded when we load tidyverse, but it could be loaded individually as well. The function `rename_with()` is like `rename()`, but allows you to use functions within it. Use `rename()` if you want to do a one-to-one change in column names.


## Removing unecessary data

To simplify the dataset, let's remove columns and rows we don't need. We will now rename the dataset as "cleandat" so that we maintain the raw data unaltered, and save all changes and additions into the clean dataset. The columns we select will correspond to the analyses we're interested in. If data are missing, we may want to remove those observations from the dataset.

```{r reduce-dataset}
#| eval: true
#| output: false

# let's see whether all the observations have flipper length
rawdat2 %>% 
  filter(is.na(flipper_length_mm))

# keep only columns of interest
# use select to remove columns (an alternative approach)
# remove observations (rows) missing flipper length
cleandat1 <- rawdat2 %>% 
  select(individual_id, species, region, island, stage, clutch_completion, 
         date_egg, flipper_length_mm, culmen_length_mm, body_mass_g,sex, 
         comments) %>% 
  select(!stage & !region) %>% 
  filter(!is.na(flipper_length_mm))

```


## Modifying variables

Often, variables that are characters need to be formatted to make them easier to use for analyses. We'll rename the dataset again to keep the original data saved.

```{r modify-variables}
#| eval: true
#| output: false

# look at species names
unique(cleandat1$species)

# remove the scientific name
cleandat2 <- cleandat1 %>% 
  mutate(species = str_remove(species, " \\(Pygoscelis adeliae\\)") %>% 
           str_remove(" \\(Pygoscelis papua\\)") %>% 
           str_remove(" \\(Pygoscelis antarctica\\)"))

# look at new species names
unique(cleandat2$species)

```

### Flagging usability of data

If it looks like some records may not be usable based on some criteria, it may be a good idea to create an indicator variable (yes/no) that identifies which record may not be usable for a specific analysis. If needed, we can do this using the `mutate` and `case_when` functions.

```{r usability}
#| eval: true
#| output: false

# look at the comments to assess if some data may not be usable for analysis  
unique(cleandat2$comments)

# create an indicator variable for whether the sex variable is usable
cleandat3 <- cleandat2 %>% 
  mutate(usable_sex_data = case_when(
    str_detect(comments, "No blood sample obtained") ~ "no",
    str_detect(comments, "Sexing primers did not amplify") ~ "no",
    TRUE ~ "yes"))

# let's see if this flag works
cleandat3 %>% 
  distinct(usable_sex_data, sex)

```

[**Note**]{style="color:red;"}: While in this example the number of unique comments to look through was relatively small and we could easily decide which ones to use for coding the "usable" columns as "yes" or "no", in real life it is very impractical and not advised to store any critical information in a general free-form "comments" column, especially if that information pertains to the quality or usability of the data. The best practice is to maintain a metadata spreadsheet that contains all of the sampling information for a given sampling point, including comments, but that would also contain a column with a "yes" or a "no" value so that the analyst can quickly filter data without having to weed through long comments or make decisions on what they may mean. Just something to keep in mind!</p>

At times, though, we can create a `is-the-data-usable (Yes/No)` column based on some variables (but not free-form comments!). For example, if one had sample-site specific `wind`, `precipitation`, `temperature`, `time of day` information, it would be easy to generate a "data quality" variable based on specific sampling conditions (e.g., if wind exceeds this, exclude this data point).

## Creating new variables

We're getting close to having a dataset that will be easier to use for analysis! Now let's say we would like to create a few additional columns to document: 1) culmen to body mass index, 2) flipper group categories based on flipper length, and 3) body size category identifying if the penguins were small, medium, or large based on their weight. We can do all three of these things using the `mutate()` function.

```{r new-variables}
#| eval: true

# create a culmen to body mass index
# assign a flipper length category
# create another group based on weight class, and let's make sure that we do not assign a class to missing data
cleandat4 <- cleandat3 %>% 
  mutate(body_mass_kg = body_mass_g/1000,
         culmen_bodymass_index = round(culmen_length_mm/body_mass_kg, 2),
         flipper_group = if_else(flipper_length_mm > 210, "big flips", 
                                 "small flips" ),
         weight_class = case_when(is.na(body_mass_g) ~ "unknown", 
                                  body_mass_g <= 3500 ~ "small", 
                                  body_mass_g < 4300 ~ "medium", 
                                  TRUE ~ "large"))

```

**Note:** The `round()` function may not work as you expect. See the help file for how it rounds off at 5. If you'd like a different approach, you can use the `round_half_up()` or `round_to_fraction()` functions in the `janitor` package.

## Formatting calendar dates and times

The package `lubridate`, which is part of the `tidyverse` collection of packages, contains many useful functions to format and manipulate date and time variables. Some functions (e.g., `as.Date()`) are part of base R, while others are part of the `lubridate` package.

```{r useful-date}
#| echo: false

useful_fct_3<-data.frame(Function = c("as.POSIXct()",
                                      "as_date() or as.Date()",
                                      "year()",
                                      "month()",
                                      "hour()"),
                         Purpose = c("Format date and time as POSIXct object",
                                     "Format object as a date",
                                     "Extract year from POSIXct object",
                                     "Extract month from POSIXct object",
                                     "Extract hour from POSIXct object"))

kable(useful_fct_3, caption = "Useful functions to format dates and times") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

[Here](https://www.neonscience.org/resources/learning-hub/tutorials/dc-convert-date-time-posix-r) you can find a useful tutorial that explores and presents some of these functions.

Let's first add a "time" variable to our dataset. The time variable will be in total number of seconds from 00:00:00. We will learn how to re-format this in proper time stamp (hour:minute:second) so that it is more intuitive to work with and visualize. Times in seconds frequently happen with GPS Location data, for example. It isn't a bad idea to have a way to convert them in your toolbox!

The code that we will type next is only used to generate a time variable. Don't worry too much about the specifics of the code since this is not something that you would have to do if you were receiving data that already contains both a *date* and a *time* column. This is only done to illustrate how to work with both dates and times, since our original penguin data only contains dates.

```{r generate-time}
#| eval: true
#| output: false

# define the start and end dates/times for the range of sampling times (in UTC, i.e., 3 hours ahead of the time zone where penguins were observed - see note below)
start_time <- 9*60*60 # 9 am in seconds from midnight
end_time <- 21*60*60 # 9 pm in seconds from midnight

# Create a sequence of all possible timestamps within the range, e.g., by 10 minute segments
full_time_sequence <- seq(start_time, end_time, 10*60)

# For each record, sample a random number from the full time sequence created above
cleandat5 <- cleandat4 %>% 
  mutate(time_sec = sample(full_time_sequence, n(), replace=TRUE)) 

# check variable
cleandat5$time_sec

```

We have now generated a new variable `time_sec` which reflects the time of day in seconds from the starting point 00:00:00. We will learn below how to re-format those times so that they are in the appropriate time zone.

*Note*: Time zones are important in R!!! If not specified, the dates and times will be assigned the time zone of your computer system, which may be misleading if the data was collected somewhere completely different (e.g., penguins in Antarctica!).

Let's format our new time variable to make it more usable in analyses and visualizations.

```{r format-time}
#| eval: true
#| output: false

# check the timezone of your computer
Sys.time()

# set date_egg as the date
# create a date and time variable. Note that although the penguins are from the Palmer LTER in Antarctica, which is in UTC-03, data were collected in UTC times and will need to be converted to UTC-03.  
# convert to proper time zone UTC-03 
cleandat6 <- cleandat5 %>% 
  mutate(date = as_date(date_egg),
         date_time_utc = as_date(date_egg, format = "%Y-%m-%d") %>% 
           paste("00:00:00") %>% 
           as.POSIXct(format="%Y-%m-%d %H:%M:%S", tz = "UTC") + time_sec,
         date_time_utc03 = as.POSIXct(date_time_utc, tz = "Etc/GMT+3"))
                                    

# look at the first few entries
cleandat6 %>% 
  select(date, time_sec, date_time_utc, date_time_utc03) %>% 
  head(n = 10)

```

**Note:** From R [help file](https://stat.ethz.ch/R-manual/R-devel/library/base/html/timezones.html) on time zones : "Most platforms support time zones of the form ‘⁠Etc/GMT+n⁠’ and ‘⁠Etc/GMT-n⁠’ (possibly also without prefix ‘⁠Etc/⁠’), which assume a fixed offset from UTC (hence no DST). Contrary to some expectations (but consistent with names such as ‘⁠PST8PDT⁠’), negative offsets are times ahead of (East of) UTC, positive offsets are times behind (West of) UTC."

Now that we've formatted our date and time variable as POSIXct object with the appropriate time zone, we can extract all sorts of useful information very easily (e.g., hour(), month(), year()).

```{r extract-time}
#| eval: true
#| output: false

# study year
year(cleandat6$date_time_utc03)

# use the mutate function to create a year column in the cleandat dataframe
cleandat7 <- cleandat6 %>% 
  mutate(year = year(date_time_utc03)) 

```

**Exercise:** Take a few minutes to experiment extracting different types of information from the POSIXct object. Are there situations where you see this could be useful (e.g., `hour()`)? What happens if you run `as_date()` on a date and time POSIXct object?

## Long and wide data

One of the principles of tidy data is for it to be "long" (i.e., each observation has a row). However, it may make sense to collect data in a "wide" format. There may also be calculations that are easier to do in a wide format. The "pivot_" functions allow us to move between long and wide formats.

```{r pivot}
#| eval: false


# make body mass wide by year
widedat <- cleandat7 %>% 
  select(individual_id, body_mass_kg, year) %>% 
  pivot_wider(names_from = year,
              values_from = body_mass_kg,
              names_prefix = "year_")

# look at new dataset
widedat

# convert it back to long
longdat <- widedat %>% 
  pivot_longer(cols = starts_with("year_"),
               names_to = "year",
               values_to = "body_mass_kg",
               names_prefix = "year_")

# look at new dataset
longdat

```

## Saving data

Now that we've cleaned our data, let's save it to our project. That way, if we want to put down our work and come back later, we can start with clean data and not re-do the steps. We can also give this dataset to collaborators.

```{r write-data}
#| eval: false

# write to a csv file
write_csv(cleandat7, "data/clean_penguindat.csv")

```

If we want to import the data into a script, we can use the `read_csv()` function, as demonstrated above.

# Creating data summaries

Creating data summaries is often the first step of a thorough Exploratory Data Analysis (EDA). The `group_by()` and `summarise()` (or `summarize()`, same function, different spelling) functions are some of the most useful functions you'll find in R as they are very intuitive and efficient. We will summarize the penguin data to illustrate what can be done with these two functions. 

Let's compare flipper length and body mass across species. The `group_by()` function allows summarizing information for one or multiple nested groups (e.g., species alone, or species within islands, etc.). The `summarise()` function will allow creating summaries such as mean, standard deviation, sample size, for the groups we defined with the `group_by()` function.

**Note**: If there are missing data in a column, the output of a summary will return NA unless it is specified that the NAs need to be ignored from the calculation.

```{r summarize}
#| eval: true
#| output: false

# remind ourselves what's in our dataset
glimpse(cleandat7)

# minimum flipper length by species
cleandat7 %>% 
  group_by(species) %>% 
  summarise(min_flipper_length = min(flipper_length_mm))

# maximum flipper length by sex
cleandat7 %>% 
  group_by(sex) %>% 
  summarise(max_flipper_length = max(flipper_length_mm))

```

**Exercise:** The maximum flipper length summary contains NA values for sex. Generate this summary without those values. There are at least two ways to find these NAs in the dataset.  

We can combine several functions in the same statement to get a more complete summary of flipper lengths across species.

```{r summary-table}
#| eval: true
#| output: false

flipper_length_summary <- cleandat7 %>% 
  group_by(species) %>% 
  summarise(mean_flipper_length = mean(flipper_length_mm),
            sd_flipper_length = sd(flipper_length_mm),
            min_flipper_length = min(flipper_length_mm),
            max_flipper_length = max(flipper_length_mm),
            N=n())

# print the summary
flipper_length_summary

```

Above we created a summary of mean, standard deviation, minimum, and maximum flipper length. There is no limit on the number of functions that we can calculate within the `summarise()` function.

But wait, we forgot to calculate the standard error which is a metric that we may need for calculating confidence intervals! You can use the `mutate()` function to add that measure to the summary table previously created.

**Note:** Another way to print output is to put parentheses around it

```{r summary-mutate}
#| eval: true
#| output: false

# add standard error column
(flipper_length_summary2 <- flipper_length_summary %>% 
  group_by(species) %>% 
  mutate(se_flipper_length = sd_flipper_length/sqrt(N)))


```

**Exercise**: Let's say that we want to summarize flipper length by species AND island. Let's take a few minutes to try creating different summaries based on multiple grouping variables.

**Hint**: the grouping variables can be added within the `group_by()` function separated by a comma. You can use the `names()` or `glimpse()` function to remember the name of the different variables available to use.

# Data visualization

Now that we have a clean dataset and have created useful summaries, we can now think of how to best visualize the data. We will be using the `ggplot2` package to demonstrate various data visualization options. While it is certainly possible to create high-quality figures in base R, `ggplot2` provides a powerful way to create and customize figures in an intuitive way (after getting comfortable with the syntax and structure!).

## Key data visualization principles

[Rougier et al.](https://doi.org/10.1371/journal.pcbi.1003833) present ten simple rules for better figures in their 2014 paper:

1. Know your audience
2. Identify your message
3. Adapt the figure to support the medium (e.g., presentation, report)
4. Captions are not optional
5. Do not trust the defaults
6. Use color effectively
7. Do not mislead the reader/viewer
8. Avoid "chartjunk"
9. Message > beauty
10. Get the right tool

R can be the right tool for many data visualizations. Start by thinking about your audience, message, and medium. This can help you decide whether R is a good fit.

## Main elements of a ggplot figure

Plots in `ggplot` are built as a series of layers sequentially added in an intuitive way. Let's think about how we would graph a figure by hand. First, we would think of the basic component of our figure or in other words the things that we want to represent (i.e., the **data**). We would then think of what specifically we want to represent (i.e., **The Xs, the Ys, etc.**), and how (i.e., **the type of chart**). `ggplot` uses this logic to sequentially add complexity and layers to plots with limitless options!

```{r ggplot-elements}
#| echo: false

ggplot_elements <- data.frame("Element" = c("Basics",
                                            "Aesthetics",
                                            "Geometry",
                                            "Labels and legends",
                                            "Themes",
                                            "Scales",
                                            "Faceting",
                                            "Statistics",
                                            "Coordinate systems"),
                              "Definition" = c("Data to be mapped",
                                               "Aesthetics of the graph such as X and Y (if more then one variable), grouping variables, or colours or shape",
                                               "Defines the type of plot",
                                               "Axis and legend labels",
                                               "Define the appearance of the plot",
                                               "Scales for the X and Y axis",
                                               "Divides the plot into subplots",
                                               "Builds new variables (e.g., regression line) to plot",
                                               "Defines the relationship between X and Y"))

kable(ggplot_elements, caption = "Elements of a plot done in ggplot") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

For each of these elements, you can find a detailed description on the [ggplot2 cheatsheet](https://rstudio.github.io/cheatsheets/data-visualization.pdf). We will give some examples of the types of figures that we can do in ggplot, and how to add layers and complexity without having to add so many lines of code that it becomes cumbersome! Let's explore some of the syntax using examples for one variable and two variables.

## One-variable figures

We'll start with our cleaned dataset, but let's save it to a final name, cleandat. That way, if we change the cleaning steps above, we just need to update the number here without changing all of our code for figures.

```{r data-glimpse}
#| eval: true
#| output: false

# save final dataset with consistent name
cleandat <- cleandat7

# remind ourselves of the dataset
glimpse(cleandat)

```

We may want to visualize the number of records that we have for each of the islands. Let's first set up the basics, or canvas, for our plot and specify the data.

```{r base-plot}
#| eval: true

ggplot(data = cleandat)
```

This is pretty much a blank slate on which we can add *aesthetics* (fancy word to mean: what is it that we want to show on our figure?). Let's first take a look at the list of possible *aesthetics*.

```{r aesthetic-table}
#| echo: false

aes_options <- data.frame("Aesthetics" = c("x",
                                           "y",
                                           "size",
                                           "alpha", 
                                           "fill", 
                                           "colour",
                                           "shape"),
                          "Description"=c("Variable to be plotted on the X axis",
                                          "Variable to be plotted on the Y axis",
                                          "Size of the point, column or line",
                                          "Transparency of the object",
                                          "The fill colour of a column area",
                                          "The colour for points and lines, or the outline colour for columns and areas",
                                          "The shape of the point when making a scatter plot"))

kable(aes_options, caption = "Options to be passed into the `aes` statement") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

We add different elements to the blank canvas using the **plus** sign. Let's count how many records we have per island in our cleaned dataset. **Island** will thus be on the **x-axis**, which is our only variable in this plot.

```{r add-x}
#| eval: true

ggplot(data = cleandat) + 
  aes(x = island)
```

Adding `aes(x = island)` simply added the x-axis to our blank canvas. We can now fill it by specifying the type of plot that we want to make with this. For simply visualizing the number of records per island we can do a barchart, which we specify using one of the available *geometries* as noted on the [ggplot2 cheatsheet](https://rstudio.github.io/cheatsheets/data-visualization.pdf).

```{r add-geom}
#| eval: true

ggplot(data = cleandat) + # blank canvas
  aes(x = island) + # define the X-axis
  geom_bar() # type of graph

```

We're off to a great start! But this figure could show more information, such as the number of records for each species on each of these islands. Recall the description of the *aesthetics* options, one being **fill** which can indicate the fill color of the bar based on a categorical variable.

```{r bar-fill}
#| eval: true

ggplot(data = cleandat) +
  aes(x = island, fill = species) +
  geom_bar()

```

Let's add labels and make the graph look a little bit nicer by removing the grey background color. Labels can be added using the `labs()` function, and the theme can be adjusted to change the overall appearance of the figure. Here, we use a black-and-white theme, but there are many available.

```{r label-plot}
#| eval: true

ggplot(data = cleandat) +
  aes(x = island, fill= species) +
  geom_bar() +
  labs(title = "Penguin species per island",
       x = "Island",
       y = "Number of penguins",
       fill = "Species") + # "fill="Species" will change the legend title for the fill aesthetics 
  theme_bw()

```

This figure is starting to look pretty good! You can spend time exploring different themes and taking a look at what the output looks like. 

An additional thing that we can do is adjust the color. We can use the function `scale_fill_brewer()` to pick a different color scheme. The available color palettes can be viewed in [RColorBrewer palette](https://colorbrewer2.org/#type=sequential&scheme=BuGn&n=3). This tool offers an opportunity to select only colors that are colorblind safe or printer friendly, among other things.

```{r plot-colors}
#| eval: true

ggplot(data = cleandat) +
  aes(x = island, fill= species) +
  geom_bar() +
  labs(title = "Penguin species per island",
       x = "Island",
       y = "Number of penguins",
       fill = "Species") + 
  theme_bw() + 
  scale_fill_brewer(type = "qual", palette = 3)

```

Often it is more desirable to look at bars that aren't stacked but that are side-by-side. The *position* of the bars can be adjusted with the `geom_bar()` function. Let's add this to our figure above since it looks so nice already!

```{r side-bars}
#| eval: true

ggplot(data = cleandat) +
  aes(x = island, fill= species) +
  geom_bar(position = position_dodge(preserve = "single")) +
  labs(title = "Penguin species per island",
       x = "Island",
       y = "Number of penguins",
       fill = "Species") + 
  theme_bw() + 
  scale_fill_brewer(type = "qual", palette = 3)

```

**Exercise**: Now it is your turn to play with single-variable plots! Refer to the [ggplot2 cheatsheet](https://rstudio.github.io/cheatsheets/data-visualization.pdf) and try making a histogram of the penguins body weights to see how the weights are distributed across the dataset overall. Then, add complexity to this figure by visualizing how the distribution of body weight varies across species! Hint: if the species-specific bars are overlapping, you can add transparency using the alpha aesthetic within the geometry function.

As you can see, the options are limitless!

## Two-variables figures

In most instances we want to show relationships between two variables, sometimes we also want to show how that relationship varies across groups. We will demonstrate this with the penguin dataset by building a figure of the relationship between penguins flipper length and body mass.

We've already seen how to build a figure from the basic level (data only) to a nice figure with labels, titles, and better colors, so we can go ahead and build a nice scatterplot right away. The geometry to use in that instance will be `geom_point()` instead of `geom_bar()`. Here will have a **X** and a **Y** variable to pass to the `aes()` function.

```{r scatterplot}
#| eval: true

ggplot(data = cleandat)+
  aes(x = body_mass_g, y = flipper_length_mm) + 
  geom_point() +
  labs(title = "Relationship between flipper length and body mass",
       x = "Body mass (g)",
       y = "Flipper length (mm)") +
  theme_bw()

```

Above we see the relationship using the full dataset (i.e., all species across all islands). We could follow a similar approach as above to show how that relationship varies across species. We could use different colors and/or different shapes to highly species-specific patterns. Let's experiment with this and use the same colour palette as above!

```{r scatterplot-colors}
#| eval: true

ggplot(data = cleandat)+
  aes(x = body_mass_g, y = flipper_length_mm, color = species) + 
  geom_point() +
  labs(title = "Relationship between flipper length and body mass",
       x = "Body mass (g)",
       y = "Flipper length (mm)",
       color = "Species") +
  theme_bw() +
  scale_colour_brewer(type = "qual", palette = 3)

```

Interesting! Intuitively our eyes are trying to draw regression lines through the cloud of points. It is possible to add these lines on the figure using the `geom_smooth()` function.

```{r plot-regression}
#| eval: true

ggplot(data = cleandat) +
  aes(x = body_mass_g, y = flipper_length_mm, color = species) + 
  geom_point() +
  labs(title = "Relationship between flipper length and body mass",
       x = "Body mass (g)",
       y = "Flipper length (mm)",
       color = "Species") +
  theme_bw() +
  scale_colour_brewer(type ="qual", palette = 3) +
  geom_smooth(method = "lm", se = FALSE, formula = 'y ~ x') 

```

**Note:** se = TRUE would show confidence limits based on the standard error. The color follows the same as the cloud of points because it's in the initial aes and does not need to be added again here. If you specify the color within `geom_point()` or `geom_smooth()` using aes(), it will only apply to that layer.

If you would like you can take a moment to play with other variables and make additional figures. Also note that this is a pretty clean dataset and straight regression lines seem to fit pretty well. This isn't always the case, however, and the figures do not replace thorough statistical tests and models!

**Exercise**: Repeat the same figure as above but show `species` as different colors and `islands` as different shapes. What do you notice?

## Facets!

At times we may want to display complex information for several categories simultaneously, and adding all of that complexity to one graph leads to figures that can be difficult to interpret. Using *facets* we can display the same figure in different panels based on the values of a categorical variable. For example, instead of using different shapes for the different islands, we could use `facet_wrap()` function as below.

```{r facets}
#| eval: true

ggplot(data = cleandat) +
  aes(x = body_mass_g, y = flipper_length_mm, color = species) +   
  geom_point() +
  labs(title="Island comparision of flipper length and body mass by species",
       x = "Body mass (g)",
       y = "Flipper length (mm)",
       color = "Species") + 
  theme_bw() +
  scale_colour_brewer(type = "qual", palette = 3) +
  geom_smooth(method = "lm", se = FALSE, formula = 'y ~ x') +
  facet_wrap(~island)

```

**Exercise**: Try adding `scales="free"` argument within `facet_wrap()` and see what happens. Would this be a useful figure if the purpose was to compare the different islands? Are there cases where it would be useful?

## Interactive plots

The `plotly` package provides functionality for making interactive plots that has tools for zooming in and out, panning, hovering and among others. While this package has many different ways to create these plots with different level of functionality, the simplest is the `ggplotly()` function which adds these tools to ggplots. This can be very useful for exploring data.

```{r plotly-start}
#| eval: true
#| warning: false

# install and load the plotly package
# install.package("plotly") # delete the '#' to install
library(plotly)

```

There are two ways to run the `ggplotly()` function. First, simply wrap the `ggplotly()` function around the chain of functions used to make a ggplot.

```{r plotly-wrap}
#| eval: true

ggplotly(
  ggplot(data = cleandat) +
  aes(x = body_mass_g, y = flipper_length_mm, color = species) +   
  geom_point() +
  labs(title="Island comparision of flipper length and body mass by species",
       x = "Body mass (g)",
       y = "Flipper length (mm)",
       color = "Species") + 
  theme_bw() +
  scale_colour_brewer(type = "qual", palette = 3) +
  geom_smooth(method = "lm", se = FALSE, formula = 'y ~ x')
)

```

The other is to name the chain of functions and wrap that named object in the `ggplotly()` function

```{r plotly-name}
#| eval: true
#| output: false

fliplen_bodmass_plot <-  ggplot(data = cleandat) +
  aes(x = body_mass_g, y = flipper_length_mm, color = species) +   
  geom_point() +
  labs(title="Island comparision of flipper length and body mass by species",
       x = "Body mass (g)",
       y = "Flipper length (mm)",
       color = "Species") + 
  theme_bw() +
  scale_colour_brewer(type = "qual", palette = 3) +
  geom_smooth(method = "lm", se = FALSE, formula = 'y ~ x')

ggplotly(fliplen_bodmass_plot)

```

## Exporting plots for publication

One of the easiest way to save and export plots is to use the `ggsave()` function. Use the `?ggsave` to obtain more information on how the function works. By default, the latest plot displayed will be saved, but if a previously displayed plot had been saved as an R object, the name of that R object can be specified to save that one instead of the last one. Options available here include `dpi=...`, `width=...` and `height=...` for our figure. This is where we can make the final adjustments depending if the plot is to be used in a powerpoint presentation, or a word document, for example.

Let's save one figure, and then play with these format options to see how it changes the appearance and quality of the exported image. Often it is by trial and errors that you will find the best format for your specific situation!

```{r ggsave}
#| eval: false

ggsave("outputs/penguins_data_exploration.jpg",
       plot = fliplen_bodmass_plot,
       dpi = 300, height = 5, width = 7, units = "in") 

```

# Conclusion

The goal of this workshop was not to go through each possible data summary tool or plot example, but rather to use a simple dataset to demonstrate some of the main tools that can be used for tidying data in R, and demonstrate with a few examples the structure of the ggplot syntax. There are plenty of resources available to expand on the material presented here, and we hope that as a participants you will feel it easier to continue exploring and applying these tools in to tackle your data analysis tasks!


# Your turn to experiment!

Now you will spend the rest of our time together implementing some of these tools with your own data. But how to get started?!!

Here is a quick guide to get you started based on the steps we have followed with the palmer penguins data, but feel free to go on your own as you see fit!

1.  Start a new project and R script.
2.  Load libraries (`tidyverse` and others as needed)
3.  Read your data in R using the function that correspond to your file type (`read_csv`(),`read_excel`())
4.  Take a first look at the data using `glimpse()`,`str()`,`dim()`,`nrow()`, etc.
    a.  Note the data structure and data types.
    b.  Note the dimension of your dataset, the number of rows and number of columns
5.  Look at the data summary (`summary()`) and identify if there are any missing data or anything else that looks unusual for some of the variables.
6.  Use some of the functions that we have used below to tidy your data
    a.  Do you need to simplify column names?
    b.  Can you remove some of the columns to form a smaller dataframe?
    c.  Do you need to created a binary variable coding if some variables are usable or not?
    d.  Do you need to create some new variables (`mutate()`)?
    e.  Do you need to filter out some of the data (`filter()`)?
7.  Now you can do some summaries using the `group_by` function if you have grouping variables, and the `summarise` function.
8.  Take time to explore a variety of plot types, even if you wouldn't necessarily use them all, but just to get more experience with the `ggplot2` syntax and the breath of available types of plots.
9.  Ask questions! We're here to help, and if there is something that we have not covered in the workshop material but that you would like to learn how to do, we will be happy to help you out!

Enjoy!

# Useful cheatsheets

[ggplot2](https://rstudio.github.io/cheatsheets/data-visualization.pdf)

[dplyr](https://nyu-cdsc.github.io/learningr/assets/data-transformation.pdf)

[tidyr](https://rstudio.github.io/cheatsheets/tidyr.pdf)

[lubridate](https://rstudio.github.io/cheatsheets/lubridate.pdf)

[stringr](https://rstudio.github.io/cheatsheets/strings.pdf)

[more cheatsheets!](https://posit.co/resources/cheatsheets/)

# Additional resources

[More on tidy data](https://tidyr.tidyverse.org/articles/tidy-data.html)

[More on ggplot2](https://ggplot2.tidyverse.org/)

[dplyr for Gen Z](https://hadley.github.io/genzplyr/)
