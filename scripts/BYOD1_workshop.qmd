---
title:    "Bring Your Own Data (BYOD) Series"
subtitle: "Summarizing and Visualizing Your Data in R"
author:   "MN-DNR Biometricians"
date:     today
engine:   knitr
format: 
  html: 
    highlight: tango
    css: camp_style.css
    number-sections: true
    fontsize: 14pt
    toc: true
    mainfont: Calibri
    monofont: Aptos Mono
---

```{r fonts}
#| echo: false
htmltools::tagList(rmarkdown::html_dependency_font_awesome())
```

```{css your-turn-callout}
#| echo: false
{
}
.YourTurn {
  margin:        0em;
  border:        2px solid #003865;
  border-left:   5px solid #003865; 
  border-radius: 10px;
}
.YourTurn-header {
  margin-left:   0em;
  margin-top:    0em;
  margin-bottom: 0em;
   /* align-items: center; */
   /* vertical-align: text-bottom; */
  padding-left:   10px;
  padding-top:    5px;
  padding-bottom: 0px;
  background-color: #0080A6;
  border: 2px solid #003865;
  border-left: 10px solid #003865; 
  border-bottom: 1px solid #003865; 
  border-top-right-radius: 10px;
  color: white;
  font-size: 1.3em;
  font-weight: bold;
  /* background-size: 25px;
  background-repeat: no-repeat;
  background-position: 10px center;
  background-image: url("fa-code-icon.png"); */
}
.YourTurn-container {
  border: 2px solid #003865;
  border-left: 10px solid #003865; 
  border-top: 1px solid #003865; 
  padding-top: 5px;
  padding-left: 10px;
  padding-right: 10px;
  padding-bottom: 5px;
  color: black;
  background-color: white;
  border-bottom-right-radius: 10px;
}
```

# Workshop goals

The goal of this workshop is to provide the tools and hands-on experience necessary so that participants are comfortable importing and tidying up their data in R, creating summaries, and generating publication-quality figures. If time allows we could also explore simple statistical models during the practice section of the workshop.

Throughout this workshop we will use the syntax of the `tidyverse` group of packages, which offers an efficient way of coding and building complexity in data manipulation steps in an organized and structured way.  

The content presented in this document was inspired by some of the courses presented at [MDH R Camp](https://tidy-mn.github.io/R-camp-penguins/). We encourage all participants to take a look at the material generously shared by MDH by following that link; it contains excellent instructional videos and coding examples.

# Setting-up our work space

Let's set up our work space for the day by first creating a new RStudio project for this workshop. All of the scripts and data used in the workshop should be saved to that project so that everything can be easily referenced in the future.

We will now take a few moments to demonstrate how to set-up a new RStudio project. When that is done, we'll create folders for data, scripts, and outputs in our project folder. Finally, let's create a new R script. We'll name it penguins_data_demo.r, and you can chose the names that you want to use. This script will contain everything that we will demonstrate in this class with a dataset that we will provide (penguins!).

# Loading our tools

## Packages

In this section, we will learn about the different packages that we will be using in this workshop, and will explore different functions for reading files into r.

```{r packages-info}
#| eval: true
#| warning: false
#| echo: false

# load table package
library(kableExtra)

# create dataframe of package info
packages_info <- data.frame(
  Package = c("readr", "readxl", "tidyr", "dplyr", "stringr", "janitor", "ggplot2", "tidyverse"),
  Purpose = c("Read delimited files","Read excel files", 
             "Tidy messy data", 
             "Format and summarize data tables",
             "Format character strings",
             "More data cleaning (not in the tidyverse)",
             "Create elegant data visualisations using the grammar of graphics",
             "Load many of the packages in the tidyverse")
)

# visualize table
kable(packages_info, caption = "Essential R packages for tidying and visualizing data") |>
  kable_styling(bootstrap_options = c("striped", "hover"))

```

Let's load packages!

```{r load-packages}
#| eval: true
#| warning: false

# load packages in the tidyverse (e.g., readr, readxl, tidyr)
library(tidyverse) 
library(readxl)

# we can check what packages were loaded
names(sessionInfo()[["otherPkgs"]])
```

## Loading data

Let's load a few datasets in different formats. First, you'll need to download the data to your computer and save it in a folder called "data" that's inside of your RStudio project.

[penguin_data_final.xlsx](https://github.com/MN-DNR-R/BYOD_Seminar1_Data_Summaries_and_Visualization/raw/refs/heads/main/data/penguin_data_final.xlsx)

Since we stored the data inside a folder in the project, the file path is very simple. If you need to load data that's stored outside the project, you'll need to specify the whole file path.

```{r load-excel}
#| eval: false

# load a .xlsx file into your R environment
mydat_xls <- read_excel("data/penguin_data_final.xlsx") 

```

Next, let's try loading data from a url. We would use the read_csv function for data on our computer as well.

```{r load-url}
#| eval: false

# load a .csv file into your R environment
mydat_csv <- read_csv("https://raw.githubusercontent.com/MN-DNR-R/BYOD_Seminar1_Data_Summaries_and_Visualization/refs/heads/main/data/penguin_data_final.csv?token=GHSAT0AAAAAADGVPTNKVJGII66G3RODGYUA2JGFBOA") 

```

A summary of the data is generated when you use the "read_" functions. You can also look in the environment and make sure that the files were loaded properly! We will also demonstrate ways to explore your data in the next section.

## Data from a package

Some datasets are stored inside of R packages. We will use one that's in a package for the rest of our demonstration.

The `palmerpenguins` package was created by Allison Horst, Alison Hill, and Kristen Gorman as a tool for teaching data manipulation and analysis techniques in R. In the following sections we will explore the data available within this package, and we will learn to manipulate and visualize the data using a variety of tools and techniques. Below you will use the `citation()` command line to get the full citation for the package!

The library contains two datasets which can be called without having to read them manually (as we previously did with the .csv or .xlsx files). One which contains raw penguins data (penguin_raw), and the second file (penguins) is the simplified (i.e., clean) dataset. 

You also can read more about the package here: [PalmerPenguins](https://allisonhorst.github.io/palmerpenguins/)

```{r palmer}
#| eval: true
#| warning: false
#| message: false

# install package (delete the '#' to run the line and use the '#' after you've installed the package)
# install.packages("palmerpenguins")

# load library into your R environment. 
library(palmerpenguins)

# get information about the palmerpenguins package:
?palmerpenguins

# Get proper citation for package if we were to be using this in publication. Note that you can use the "citation" function for any of the packages used in an analysis, which is very useful when writing the methods in a report or peer-reviewed manuscript. 
citation("palmerpenguins")

# let's save the raw data in our environment
rawdat <- penguins_raw
```

# Getting to know your data

## Data exploration functions

There are many functions available to explore the data. Here is a list of the main ones that we will be using in this workshop.

```{r useful-functions}
#| eval: true
#| warning: false
#| echo: false

useful_fct_1 <- data.frame(Function = c("glimpse()", "str()", "names()", "head()", "tail()"," nrow()", "ncol()", "summary()", "unique()"),
                           Purpose = c("Overview of dataframe rows, columns, column names, daraframe dimension, and a glimpse of first values", 
                                       "Succinct summary of all columns of a dataframe; similar to glimpse but within base R",
                                       "Column names",
                                       "Display first n rows of data (default is 6)",
                                       "Displays last n rows of data (default is 6)",
                                       "Number of rows",
                                       "Number of columns",
                                       "Summary of the data in each column",
                                       "Displays unique values of a variable"))

kable(useful_fct_1, caption = "Useful functions for data exploration") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))

```

## Data types

Before doing any data manipulation in R, it is important to recognize the different data types. When importing your data into R, R will automatically recognize the type of data of each column. At times, however, a specific data type may need to be set, or converted from one to another. 

For example, one could use the `as.numeric()` function to convert a column to *numeric*, or `as.character()` to convert to *character*. When doing such data conversion, it is important to note that some elements of a vector may be converted to NAs if R recognizes that it does not correspond to the data type specified.

A good example of this is when a column that should be *numeric* contains characters (e.g., instead of an actual counts one had recorded something like *more then 100 animals*). In this case, this would return an *NA* as R does not know what to do with this information! Good data entry and quality checks prior to importing data into R are very important  to help prevent data loss.

With a *factor* data type, you can specify the order of variables, such as "low", "medium", "high". This defined order can then make data visualizations and analyses more intuitive.

```{r data-types}
#| eval: true
#| warning: false
#| echo: false

data_types <- data.frame(Type = c("Numeric", "Integer", "Logical", "Character", "Factor"),
                         Example = c("1.5, 2.6", "1, 2", "True/False", "One, Two", "Control, Treatment"))

kable(data_types, caption = "Data types supported in R") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))

```

## Examining the data

Let's use the functions from the tables above to examine what's in the dataset

```{r examine-data}
#| eval: true

# the glimpse function provides a good first overview of the data
glimpse(rawdat)

# the str function also provides a quick but complete overview of the structure of the data
str(rawdat)

# quick summary of all of the data
summary(rawdat)

# print column names
names(rawdat)

# look at first and last rows
head(rawdat, n = 10)
tail(rawdat, n = 10)

# number of rows and columns
nrow(rawdat)
ncol(rawdat)

# lists unique values (this can help identify if there are data entry errors)
unique(rawdat$Species)
unique(rawdat$Island)
unique(rawdat$Region)

# How many unique ID's do we have? How many data points do we have? Do they match?
unique(rawdat$`Individual ID`) 

```

## Data quality check

Looking at the raw data, what do you observe could be improved for facilitating some analyses and data visualizations? Is the dataset pretty clean and analysis-ready, or are these things that need to be addressed and changed so that we don't run into issues later in the process?

Here are some examples:

1.  species names are long
2.  some columns may not be necessary
3.  individual ID is not unique
4.  we may want columns that classify the data based on some criteria (e.g., a) heavy or light and b) long-, medium-, or short-billed).

# Tidying your data

## What is tidy data?

We refer to "tidy data" as a dataset where each column is a variable, each row is an observation, and each cell contains a single value and a data type that is consistent across a given column (e.g., all numeric). The `tidyr` packages contains functions for manipulating the data so that we can create a clean, tidy dataset to-be-used for all analyses.

</p>

**Note**: Recall that we will never overwrite the original dataset. The raw data should always remain in its original location and be kept unaltered. The beauty of R scripts is that we can save all data cleaning and data manipulation steps while retaining the original data

## Why is it important to tidy our data?

Taking time to tidy a dataset ensures that the data summaries, analyses, or visualizations we produce represent "true" data values to the best of our knowledge (e.g., no wrongly assigned missing data or data-entry related outliers). It is a the first and most critical step of any data analysis, and though it can be a lengthy process, it is worth the effort.

## Why tidying data in R?

Using R and R scripts ensures that the original dataset is kept, and that all data manipulation steps are documented and reproducible. This is a major advantage of tidying data in R as opposed to using spreadsheet applications such as Excel, where there is often no to little track records of changes made to a dataset.

</p>

## Introducing pipe operator

R uses many different operators which includes symbols such as <- and = along with typical arithmetic (+, -, *, /, ^) and relational (<, >, <=, >=, == ,!=) symbols. Too increase efficiency, the tidyr library uses symbols called pipe operators (%>%). These allow you to string together functions to run sequentially, moving left to right, kind of like a data assembly line. It helps keep the data tidy by creating new versions of your dataset, while keeping the original version. These pipe operators also tend to be more computationally efficient than base R, leading to quicker processing times, especially for large datasets.
```{r eval=F}
#Edit the raw data to filter for samples collected from Torgersen Island and arrange the table by the date the egg was collected

torg <- rawdat %>% filter(Island == "Torgersen") %>% arrange('Date Egg')


```
The *pipe* operator `%>%` or `|>` provides a way to efficiently chain functions together in a single statement. It is similar to saying: let's take this data, *and then* do this with it, *and then* do this, etc. So ultimately, every time that you see or use `%>%` in a script, you can think of it as being the same as if you were reading or coding *and then...*!

</p>

More background on the origin and use of the pipe operator can be found [here](https://www.datacamp.com/tutorial/pipe-r-tutorial).

## Let's clean-up some of those columns

Now that we've introduced a few basic tidy data concepts and coding tools, we can now start looking at our data more formally. There are many very useful functions to subset dataframes, re-arrange columns, create new columns, etc. Below are the ones you may run into more frequently as you learn to tidy your data in R and with the `tidy` package.

```{r useful tidy function,echo=F}
useful_fct_2<-data.frame(Function=c("filter()","select()","arrange()","mutate()"),Purpose=c("keep (or exclude) rows that meet specific criteria","select columns to keep (or to drop)","sort a dataframe based on a column's value (or several columns' values)","add new columns or update existing columns"))

kable(useful_fct_2, caption = "Useful functions to create tidy data") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))

```

### Formating variable names

```{r create tidy data, eval=F}
# Let's take a look at the data again
head(rawdat)
names(rawdat)

# lower column names
names(rawdat)<-tolower(names(rawdat))

names(rawdat)

# replace spaces; spaces in variable names are just slightly more complicated to code and it is much simpler not to have any spaces at all

names(rawdat)<-str_replace_all(names(rawdat)," ","_") # Note: str_replace_all is part of the stringr package, automatically loaded when we load tidyverse, but it could be loaded individually as well.

# further modify column names for simplicity
names(rawdat)<-str_replace_all(names(rawdat),"\\(|\\)","")
```

### Removing unecessary data, creating variables, and subsetting the dataset

```{r reduce dataset,eval=F}

# keep only columns of interest; we will now create a new clean dataset so that we maintain the raw data unaltered from previous steps, and save all changes and addition into the clean data dataset. 
cleandat <- rawdat %>% select(species,region,island,stage,clutch_completion,date_egg,flipper_length_mm,culmen_length_mm,body_mass_g,sex,comments) 

# if you wanted to remove one or severeal column it may be easier to do the following:
cleandat<- cleandat %>% select(!stage & !region) 

# remove scientific names in species
unique(cleandat$species)

cleandat$species<-str_replace(cleandat$species," \\(Pygoscelis adeliae\\)","")
cleandat$species<-str_replace(cleandat$species," \\(Pygoscelis papua\\)","")
cleandat$species<-str_replace(cleandat$species," \\(Pygoscelis antarctica\\)","")

unique(cleandat$species)

# Create unique record ID
penguins <- mutate(cleandat, 
                   unique_id = 1:n()) 

```

### Assessing usability of data

If it looks like some records may not be usable based on some criteria, it may be a good idea to create an indicator variable (yes/no) that identifies which record may not be usable for analysis. If needed, we can do this using the `mutate` and `case_when` functions.

```{r eval=F}
# Let's look at the comments to assess if some data may not be usable for analysis.  
unique(cleandat$comments)

## Create an indicator variable for the "adult not sampled" comments so that we can easilly remove them from future analyses if needed.
cleandat <-cleandat %>% mutate(usable_adult_data=case_when(comments=="Adult not sampled."~"no",comments=="Adult not sampled. Nest never observed with full clutch."~"no",TRUE~"yes"))

# We can now filter-out that comment column as it will no longer be necessary for our work. 
cleandat <- cleandat %>% select(!comments)

```

[**Note**]{style="color:red;"}: While in this example the number of unique comments to look through was relatively small and we could thus easily decide which ones to use for coding the "usable" columns as "yes" or "no", in real life it is very impractical and not advised to store any critical information in a general free-form "comments" column, especially if that information pertains to the quality or usability of the data. The best practice is to maintain a metadata spreadsheet that contains all of the sampling information for a given sampling point, including comments, but that would also contain a column with a "yes" or a "no" value so that the analyst can quickly filter data without having to weed through long comments or make decisions on what they may mean. Just something to keep in mind!

</p>

At times, though, we can create a `is-the-data-usable (Yes/No)` column based on some variables (but not free-form comments!). For example, if one had sample-site specific `wind`, `precipitation`, `temperature`, `time of day` information, it would be easy to generate a "data quality" variable based on specific sampling conditions (e.g., if wind exceeds this, exclude this data point).

### Creating new variables

We're getting close to having a dataset that will be easier to use for analysis! Now let's say we would like to create 1) a culmen to body mass index, 2) flipper group categories based on flipper length, and 3) a body size category identifying if the penguins were small, medium, or large based on their weight. We can do both of these things using the `mutate()` and `case_when()` functions.

```{r eval=F}

## Create a culmen to body mass index
cleandat <- cleandat %>% mutate(body_mass_kg=body_mass_g/1000,culmen_bodymass_index=round(culmen_length_mm/body_mass_kg,2))

## Assign flipper lenght category; the TRUE statement means that anything not assigned to "big flips" will be assigned to "small flips" in that example. 
cleandat<-cleandat %>% 
    mutate(flipper_group = case_when(flipper_length_mm > 210 ~ "big flips",TRUE ~ "small flips" ))

## Oups!! We forgot to check if we had NAs in the data before converting to a group!
filter(cleandat,is.na(flipper_length_mm))

## Let's correct that mistake and re-create the groups while accounting for NAs first.
cleandat<-cleandat %>% 
    mutate(flipper_group = case_when(is.na(flipper_length_mm)~"unknown",flipper_length_mm > 210 ~ "big flips",TRUE ~ "small flips" ))

## Using that same logic, let's create another group absed on weight class, and let's make sure that we do not assign a class to missing data. 
cleandat<-cleandat %>% mutate(weight_class = case_when(is.na(body_mass_g)~"unknown",body_mass_g<=3500~"small",body_mass_g<4300~"medium",TRUE~"large"))

head(cleandat)
```

## Formatting calendar dates and times

### Below are some useful functions for handling dates and times in R

The package `lubridate`, which is part of the `tidyverse` collection of packages, contains many useful functions to format and manipulate date and time variables. Some functions (e.g., `as.Date()`) are part of base R, while others are part of the lubridate packages.

```{r useful date time functions,echo=F,eval=F}
useful_fct_3<-data.frame(Function=c("as.POSIXct()","as_date() or as.Date()","year()","month()","hour()"),Purpose=c("format date and time as POSIXct object","format object as a date","extract year from POSIXct object","extract month from POSIXct object","extract hour from POSIXct object"))

kable(useful_fct_3, caption = "Useful functions to format dates and times") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

[Here](https://www.neonscience.org/resources/learning-hub/tutorials/dc-convert-date-time-posix-r) you can find a useful tutorial that explores and presents some of these functions.

### Let's first add a "time" variable to our dataset

The time variable will be in total number of seconds from 00:00:00. We will learn how to re-format this in proper time stamp (hour:minute:second) so that it is more intuitive to work with and visualize. Times in seconds frequently happen with GPS Location data, for example. It isn't a bad idea to have a way to convert them in your toolbox!

</p>

The code that we will type next is only used to generate a time variable. Don't worry too much about the specifics of the code since this is not something that you would have to do if you were receiving data that already contains a date and a time column. This is only done to illustrate how to work with both dates and times, since our original penguin data only contains dates.

```{r eval=F}
# Let's generate random times; this is only for illustrative purposes!!

# Define the start and end dates/times for the range of sampling times (in UTC, i.e., 3 hours ahead of the time zone where penguins were observed - see note below)
min_start_time <- 9*60*60 # 6 am in seconds from midnight
max_end_time <- 21*60*60 # 6 pm in seconds from midnight

# Create a sequence of all possible timestamps within the range, e.g., by 10 minute segments
full_time_sequence <- seq(min_start_time, max_end_time, 10*60)

# For each record, sample a random number from the full time sequence created above
cleandat <- cleandat %>% mutate(time_sec=sample(full_time_sequence,n(),replace=TRUE)) 

```

We have now generated a new variable `time_sec` which reflects the time of day in seconds from the starting point 00:00:00. We will learn below how to re-format those times so that they are in the appropriate time zone.

</p>

*Note*: Time zones are important in R!!! If not specified, the dates and times will be assigned the time zone of your computer system, which may be misleading if the data was collected somewhere completely different (e.g., penguins in Antartica!).

</p>

*Exercise*: Check what time zone your computer is set at using the `Sys.time()` function.

### Formatting dates and times

```{r eval=F}
# first set date as a set date
cleandat <- cleandat %>% mutate(date=as_date(date_egg))

# create a date and time variable. Note that although the palmer penguins dataset was collaected at the Palmer LTER in Antarctica, which is in UTC-03, data were collected in UTC times and will need to be converted to UTC-03.  
cleandat <- cleandat %>% mutate(date_time_utc=as.POSIXct(paste(as_date(date_egg,format="%Y-%m-%d"),"00:00:00",format="%Y-%m-%d %H:%M:%S"),tz="UTC")+time_sec)

# Convert to proper time zone UTC-03                                    
cleandat <- cleandat %>% mutate(date_time_utc03=as.POSIXct(format(date_time_utc,tz="Etc/GMT+3"),tz="Etc/GMT+3"))
                                    
# Note from R help file on time zones (https://stat.ethz.ch/R-manual/R-devel/library/base/html/timezones.html): "Most platforms support time zones of the form ‘⁠Etc/GMT+n⁠’ and ‘⁠Etc/GMT-n⁠’ (possibly also without prefix ‘⁠Etc/⁠’), which assume a fixed offset from UTC (hence no DST). Contrary to some expectations (but consistent with names such as ‘⁠PST8PDT⁠’), negative offsets are times ahead of (East of) UTC, positive offsets are times behind (West of) UTC."

```

### Extracting information from a date and time object

Now that we've formatted our date and time variable as POSIXct project with the appropriate time zone, we can extract all sorts of useful information very easily.

```{r eval=F}
# study year
year(cleandat$date_time_utc03)

# use the mutate function to create a year column in the cleandat dataframe
cleandat <- cleandat %>% mutate(year=year(date_time_utc03)) 

# we can extract all sorts of information, month, day, hour, minute, etc. from date time objects when they are formatted adequately

# Take a few minutes to experiment with some of these. 

```

*Exercise*: Take a few minutes to experiment extracting different types of information from the POSIXct object. Are there situations where you see this could be useful (e.g., `hour()`)? What happens if you run `as_date()` on a date and time POSIXct object?

# Creating data summaries

Creating data summaries is often the first step of a thorough Exploratory Data Analysis (EDA). The `group_by` and `summarise` functions are some of the most useful functions you'll find in R as they are very intuitive and efficient.

</p>

We will summarise the penguin data to illustrate what can be done with these two functions. Let's first take a look at the data again using `glimpse` to refresh our memory on the class of data that we have on hand.

</p>

We also will want to filter out the data we flagged as "unusable" during our data tidying exercise.

## Removing unusable and unecessary data to create our "working" dataset

```{r eval=F}
# taking a look at the data
glimpse(cleandat)

# filtering out the unusable records
cleandat <- cleandat %>% filter(usable_adult_data!="no") # using the != syntax means "not equal to", so basically here we are saying that we are keeping everything that is not equal to "no", i.e., we are keeping the "yes"!

# cleandat contains columns which for simplicity we may want to remove
cleandat <- cleandat %>% select(!usable_adult_data & !time_sec & !date & !date_time_utc)

glimpse(cleandat) # Much better!!!

# Let's save our cleaned dataset
write_csv(cleandat,"data/clean_peanguindat.csv")

```

Let's compare flipper length and body mass across species. The `group_by` function allows summarizing information for one or multiple nested groups (e.g., species alone, or species within islands, etc.). The `summarise` function will allow creating summaries such as mean, standard deviation, sample size, for the groups we defined with the `group_by` function.

</p>

**Note**: If there are missing data in a column, the output of a summary will return NA unless it is specified that the NAs need to be ignored from the calculation.

```{r read dat,eval=T,echo=F,message=F,warning=F}
# read cleandat created above; this chunk is for course developers only because we are not actually creating the script when running the report since it is to demonstrate and go over each command line with  participants. Dataset will thus be created in class, but we need to upload it if we want to print some of the summaries and graphs in lines of code that follow 
library(tidyverse)

cleandat<-read_csv("../data/clean_peanguindat.csv")

# cleandat<-read_csv("data/clean_peanguindat.csv") To read on individual computer 

```

### Summarizing flipper lengths

Let's first illustrate the tidyr syntax by creating a summary of flipper length by species. We can run this list of commands as they are, or create an R object if we wanted to store the summary using the `<-` syntax, e.g., `flipper_lt_summary <- cleandat %>% ...`.

```{r, eval=F}
cleandat %>% 
  group_by(species) %>% 
  summarise(min_flipper_length=mean(flipper_length_mm,na.rm=T))

```

Summarizing means only may not be the most satistifying if one wanted to compare the means across species. We can combine functions in that same statement, and get a more complete summary of flipper length across species.

```{r,summary table, echo=F,eval=T,include=T,message=F}
flipper_length_summary <- cleandat %>% 
  group_by(species) %>% 
  summarise(mean_flipper_length=mean(flipper_length_mm,na.rm=T),sd_flipper_length=sd(flipper_length_mm,na.rm=T),min_flipper_length=min(flipper_length_mm,na.rm=T),max_flipper_length=max(flipper_length_mm,na.rm=T),N=n())

kable(flipper_length_summary)

```

Above we created a summary of mean, standard deviation, minimum, and maximum flipper length. There is no limit on the number of functions we can calculate with thing the `summarise` statement.

</p>

But wait, we forgot to calculate the standard error which is a metric that we may need for calculating confidence intervals! You can use the `mutate` function to add that measure to the summary table previously created.

```{r}
flipper_length_summary <- flipper_length_summary %>% 
  group_by(species) %>% 
mutate(se_flipper_length = sd_flipper_length/sqrt(N))

kable(flipper_length_summary)
```

**Exercise**: Let's say that we want to summarise flipper length by species AND island. Let's take a few minutes to try creating different summaries based on multiple grouping variables.

</p>

**Hint**: the grouping variables can be added within that `group_by()` function separated by a `comma`. You can use the `names()` or `glimpse` function to remember the name of the different variables available to use.

</p>

# Data visualization

Now that we have a clean dataset and have created useful summaries, we can now think of how to best visualize the data. We will be using the `ggplot2` package to demonstrate various data visualization options. While it is certainly possible to create high-quality figures in base R, ggplot2 provides a powerful way to create and customize figures in an intuitive way (after getting comfortable with the syntax and structure!).

</p>

## Key Data Visualization principes (incomplete, need a few items here!)

## Main elements of a ggplot figure

Plots in `ggplot` are built as a series of layers sequentially added in an intuitive way. Let's think about how we would graph a figure by hand. First, we would think of the basic component or our figure or in other words the things that we want to represent (i.e., the **data**). We would then think of what specifically do we want to represent (i.e., **The Xs, the Ys, etc.**), and how (i.e., **the type of image**). `ggplot` uses this logic to sequentially add complexity and layers to plots wiht limitless options.\</\>

Below are the main elements of a plot built in ggplot:

```{r elements of ggplot, eval=T,echo=F,message=F}
ggplot_elements<-data.frame("Element"=c("Basics","Aesthetics","Geometry","Labels and legents","Themes","Scales","Faceting","Statistics","Coordinate systems"),"Definition"=c("Data to be mapped","aesthetics of the graph such as X and Y (if more then one variable),grouping variables, or colours or shape","Defines the type of plot","Axis and legend labels","Define the appearance of the plot","Scales for the X and Y axis","Divides the plot into subplots","Builds new variables (e.g., regression line) to plot","Defines the relationship between X and Y"))

kable(ggplot_elements, caption = "Elements of a plot done in ggplot") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

For each of these elements, you can find a detailed description on the [ggplot2 cheatsheet](https://rstudio.github.io/cheatsheets/data-visualization.pdf). We will give some examples of the types of figures that we can do in ggplot, and how to add layers and complexity without having to add so many lines of code that it becomes cumbersome! Let's explore some of the syntax using examples for one variable and two variables.

## One-variable figures

Recall the penguin dataset and how the data is structured.

```{r,eval=T}
glimpse(cleandat)
```

We may want to visualize the number of records that we have for each of the islands. Let's first set up the basics, or canvas, for our plot and specify the data.

```{r eval=T,message=T,echo=T}
ggplot(data=cleandat)
```

This is pretty much a blank slate on which we can add `aesthetics` (what is it that we want to show on our figure?). Let's first take a look at the list of possible `aesthetics`.

```{r, eval=T,message=F,echo=F}
aes_options <- data.frame("Aesthetics"=c("x =","y =","size =","alpha =", "fill =", "colour =","shape ="),"Description"=c("Variable to be plotted on the X axis","Variable to be plotted on the Y axis","Size of the point, column or line","Transparency of the object","The fill colour of a column area","The colour for points and lines, or the outline colour for columns and areas","The shape of the point when making a scatter plot"))

kable(aes_options, caption = "Options to be passed into the `aes` statement") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

We add different elements to the blank canvas using the **plus** sign. Let's count how many records we have per island in our cleaned dataset. **Island** will thus be on the **x axis**, which is our only variable in this plot.

```{r eval=T,echo=T,message=F}
ggplot(data=cleandat)+aes(x=island)
```

Adding `aes(x=island)` simply added the X axis to our blank canvas. We can now fill it by specifying the type of plot that we want to make with this. For simply visualizing the number of records per island we will do a barchar, which we specify using one of the available `geometries` as noted on the [ggplot2 cheatsheet](https://rstudio.github.io/cheatsheets/data-visualization.pdf).

```{r eval=T,echo=T,message=F}
ggplot(data=cleandat)+ # blank canvas
  aes(x=island)+ # define the X axis
  geom_bar() # type of graph
```

We're off to a great start! But this figure could show more information, such as the number of records for each species on each of these islands. Recall the description of the `aesthetics` options,one being **fill** which can indicate the fill colour of the bar based on a categorical variable.

```{r eval=T,echo=T,message=F}
ggplot(data=cleandat)+
  aes(x=island, fill=species)+ # species will be in colour, island will be on the X axis
  geom_bar() # type of graph
```

Let's add labels and make the graph look a little bit nice by removing the grey background colour. Labels can be added using the `labs` syntax, and the `theme` can be adjusted to change the overall appearance of the figure.

```{r eval=T,echo=T,message=F}
ggplot(data=cleandat)+
  aes(x=island, fill=species)+ # species will be in colour, island will be on the X axis
  geom_bar()+ # type of graph
labs(title="Distribution of penguin species per island in the Palmer penguins clean dataset",x="Island",y="Number of penguins",fill="Species")+ #Customizing labels; "fill="Species" will change the legend title for the fill aesthetics 
  theme_bw() # remove grey background
```

This figure is starting to look pretty good! You can spend time exploring different themes and taking a look at what the output looks like. An additional thing that we can do is adjust the colour. We can use the function `scale_fill_brewer` to pick a different colour scheme. The available colour palettes can be viewed in [RColorBrewer palette](https://colorbrewer2.org/#type=sequential&scheme=BuGn&n=3). This tools offers an opportunity to select only colours that are colorblind safe or printer friendly, among other things.

```{r eval=T,echo=T,message=F}
ggplot(data=cleandat)+
  aes(x=island, fill=species)+ # species will be in colour, island will be on the X axis
  geom_bar()+ # type of graph
labs(title="Distribution of penguin species per island in the Palmer penguins clean dataset",x="Island",y="Number of penguins",fill="Species")+ #Customizing labels; "fill="Species" will change the legend title for the fill aesthetics 
  theme_bw()+ # remove grey background
scale_fill_brewer(type="qual",palette=3)
```

Often it is more desirable to look at bars that aren't stacked but that are side-by-side. The `position` of the bars can be adjusted within the `geom_bar()` statement. Let's add this to our figure above (since it looks so nice already!) with a simple addition withih the parentheses of `geom_bar()`.

```{r eval=T,echo=T,message=F}
ggplot(data=cleandat)+
  aes(x=island, fill=species)+ # species will be in colour, island will be on the X axis
  geom_bar(position=position_dodge(preserve="single"))+ # type of graph
labs(title="Distribution of penguin species per island in the Palmer penguins clean dataset",x="Island",y="Number of penguins",fill="Species")+ #Customizing labels; "fill="Species" will change the legend title for the fill aesthetics 
  theme_bw()+ # remove grey background
scale_fill_brewer(type="qual",palette=3)
```

**Exercise**: Now it is your turn to play with single-variable plots! Refer to the [ggplot2 cheatsheet](https://rstudio.github.io/cheatsheets/data-visualization.pdf) and try making a histogram of the penguins body weights to see how the weights are distributed across the dataset overall. In a second time, add complexity to this figure by visualizing how the distribution of body weight varies across species! Hint: if the species-specific bars are overlapping, you can add transparency using the `alpha` statement within the geometry function.

```{r,eval=F,echo=F,message=F}
ggplot(data=cleandat)+
  aes(x=body_mass_g, fill=species)+ # species will be in colour, island will be on the X axis
  geom_histogram(alpha=0.65)+
  scale_fill_brewer(type="qual",palette=3)+
  theme_bw()
```

As you can see, the options are limitless!

## Two-variables figures

In most instances we want to show relationships between two variables, sometimes also show how that relationship varies across groups. We will demonstrate this with the penguin dataset by building a figure of the relationship between penguins flipper length and body mass.

</p>

We've already seen how to build a figure from the basic level (data only) to a nice figure with labels, titles, and better colours, so we can go ahead and build a nice scatterplot right away. The geometry to use in that instance will be `geom_point()` instead of `geom_bar()`. Here will have a **X** and a **Y** variable to pass to the `aes()` function.

```{r }
ggplot(data=cleandat)+
  aes(x=body_mass_g,y=flipper_length_mm)+ 
  geom_point()+ # type of graph
labs(title="Relationship between flipper length and body mass",x="Body mass (g)",y="Flipper length (mm)")+ #Customizing labels
  theme_bw() # remove grey background
```

Above we see the relationship using the whole dataset. We could follow a similar approach as above to show how that relationship varies across species. We could use different colours and/or different shapes to highly species-specific patterns. Let's experiment with this and use the same colour palette as above!

```{r,echo=T,message=F,warning=FALSE}
ggplot(data=cleandat)+
  aes(x=body_mass_g,y=flipper_length_mm,colour=species)+ 
  geom_point()+ # type of graph
labs(title="Species-specific relationship between flipper length and body mass",x="Body mass (g)",y="Flipper length (mm)",colour="Species")+ #Customizing labels
  theme_bw()+ # remove grey background
  scale_colour_brewer(type="qual",palette=3)
```

Interesting! Intuitively our eyes are trying to draw regression lines through the cloud of points. It is possible to add these lines on the figure using the `geom_smooth` function.

```{r,echo=T,message=F,warning=FALSE}
ggplot(data=cleandat)+
  aes(x=body_mass_g,y=flipper_length_mm,colour=species)+ 
  geom_point()+ # type of graph
labs(title="Relationship between flipper length and body mass by species",x="Body mass (g)",y="Flipper length (mm)",colour="Species")+ #Customizing labels
  theme_bw()+ # remove grey background
  scale_colour_brewer(type="qual",palette=3)+
  geom_smooth(method="lm",se=FALSE) # se=T would show confidence limits based on the standard error. The colour follows the same as the cloud of points and does not need to be added again here.
```

If you would like you can take a moment to play with other variables and make additional figures. Also note that this is a pretty clean dataset and straight regression lines seem to fit pretty well. This isn't always the case, however, and the figures do not replace thorough statistical tests and models!

</p>

**Exercise**: Repeat the same figure as above but show `species` as different colours and `islands` as different shapes. What do you notice?

## Facets!

At times we may want to display complex information for several categories simultaneously, and adding all of that complexity to one graph leads to figures that can be difficult to interpret. Using `facets` we can display the same figure in different panels based on the values of a categorical variable. For example, instead of using different shapes for the different islands, we could use `facet_wrap()` function as below.

```{r,echo=T,message=F,warning=FALSE}
ggplot(data=cleandat)+
  aes(x=body_mass_g,y=flipper_length_mm,colour=species)+ 
  geom_point()+ # type of graph
labs(title="Island comparision of the relationship between flipper length and body mass by species",x="Body mass (g)",y="Flipper length (mm)",colour="Species")+ #Customizing labels
  theme_bw()+ # remove grey background
  scale_colour_brewer(type="qual",palette=3)+
  geom_smooth(method="lm",se=FALSE)+ # se=T would show confidence limits based on the standard error. The colour follows the same as the cloud of points and does not need to be added again here.
facet_wrap(~island)
```

**Exercise**: Try adding the `scales="free"` argument within `the` facet_wrap() and see what happens. Would this be a useful figure if the purpose was to compare the different islands? Are there cases where it would be useful?

</p>

## Customizing plot options (incomplete, need to add text modif and point size, etc.)

```{r,echo=T,message=F,warning=FALSE}
ggplot(data=cleandat)+
  aes(x=body_mass_g,y=flipper_length_mm,colour=species)+ 
  geom_point()+ # type of graph
labs(title="Island comparision of the relationship between flipper length and body mass by species",x="Body mass (g)",y="Flipper length (mm)",colour="Species")+ #Customizing labels
  theme_bw()+ # remove grey background
  scale_colour_brewer(type="qual",palette=3)+
  geom_smooth(method="lm",se=FALSE)+ # se=T would show confidence limits based on the standard error. The colour follows the same as the cloud of points and does not need to be added again here.
facet_wrap(~island)
```

## Interactive plots

The plotly package provides functionality for making interactive plots that has tools for zooming in and out, panning, hovering and among others. While this package has many different ways to create these plots with different level of functionality, the simplest is the ggplotly() function which adds these tools to ggplots. This can be very useful for exploring data.

```{r eval=F}
#Install and load the plotly package
install.package("plotly")
library(plotly)

#There are two ways to run the ggplotly() function

#First simply wrap the ggplotly() function around the chain of functions used to make a ggplot
ggplotly(
  ggplot(data=cleandat)+
  aes(x=body_mass_g,y=flipper_length_mm,colour=species)+ 
  geom_point()+ # type of graph
labs(title="Relationship between flipper length and body mass by species",x="Body mass (g)",y="Flipper length (mm)",colour="Species")+ #Customizing labels
  theme_bw()+ # remove grey background
  scale_colour_brewer(type="qual",palette=3)+
  geom_smooth(method="lm",se=FALSE)
)

#The other is to name the chain of functions and wrap that named object in the ggplotly() function
fliplen_bodmass_plot = ggplot(data=cleandat)+
  aes(x=body_mass_g,y=flipper_length_mm,colour=species)+ 
  geom_point()+ # type of graph
labs(title="Relationship between flipper length and body mass by species",x="Body mass (g)",y="Flipper length (mm)",colour="Species")+ #Customizing labels
  theme_bw()+ # remove grey background
  scale_colour_brewer(type="qual",palette=3)+
  geom_smooth(method="lm",se=FALSE)

ggplotly(fliplen_bodmass_plot)

```

## Exporting datasets in nice format for publication

One of the easiest way to save and export plots is to use the `ggsave` function. Use the `?ggsave` to obtain more information on how the function works. By default, the latest plot displayed will be saved, but if a previously-displayed plot had been saved as an R object, the name of that R object can be specified to save that one instead of the last one. Options available here include `dpi=...`, `width=...` and `height=...` for our figure. This is where we can make the final adjustments depending if the plot is to be used in a powerpoint presentation, or a word document, for example.

</p>

Let's save one figure, and then play with these format options to see how it changes the appearance and quality of the exported image.

```{r,echo=T,message=F,warning=FALSE}
getwd() # To remind yourself of the current working directory. Plots will be saved in that directory unless specified otherwise

ggsave("../outputs/penguins_data_exploration.jpg",plot=last_plot(),dpi=300,height=5,width=7,units = "in") 

```

# Conclusion

The goal of this workshop was not to go through each possible data summary tool or plot example, but rather to use a simple dataset to demonstrate some of the main tools that can be used for tidying data in R, and demonstrate with a few examples the structure of the ggplot syntax. There are plenty of resources available to expand on the material presented here, and we hope that as a participants you will feel it easier to continue exploring and applying these tools in to tackle your data analysis tasks!

</p>

# Your turn to experiment!

Now you will spend the rest of our time together implementing some of these tools with your own data. But how to get started?!!

Here is a quick guide to get you started based on the steps we have following with the palmer penguins data, but feel free to go on your own as you see fit!

</p>

1.  Start a new project and R script.

    </p>

2.  Load libraries (`tidyverse` and others as needed)

    </p>

3.  Read your data in R using the function that correspond to your file type (`read_csv`(),`read_excel`())

    </p>

4.  Take a first look at the data using `glimpse()`,`str()`,`dim()`,`nrow()`, etc.

    a.  Note the data structure and data types.

    b.  Note the dimension of your dataset, the number of rows and number of columns

        </p>

5.  Look at the data summary (`summary()`) and identify if there are any missing data or anything else that looks unusual for some of the variables.

    </p>

6.  Use some of the functions that we have used below to tidy your data

    a.  Do you need to simplify column names?

    b.  Can you remove some of the columns to form a small dataframe?

    c.  Do you need to created a binary variable coding if some variables are usable or not?

    d.  Do you need to create some new variables (`mutate()`)?

    e.  Do you need to filter out some of the data (`filter()`)?

        </p>

7.  Now you can do some summaries using the `group_by` function if you have grouping variables, and the `summarise` function.

    </p>

8.  Take time to explore a variety of plot types, even if you wouldn't necessarily use them all, but just to get more experience with the `ggplot2` syntax and the breath of available types of plots.

    </p>

9.  Ask questions! We're here to help, and if there is something that we have not covered in the workshop material but that you would like to learn how to do, we will be happy to help you out!

    </p>

Enjoy!

# Useful cheatsheets

[ggplot2](https://rstudio.github.io/cheatsheets/data-visualization.pdf)

[dplyr](https://nyu-cdsc.github.io/learningr/assets/data-transformation.pdf)

[tidyr](https://rstudio.github.io/cheatsheets/tidyr.pdf)

# Additional resources

[More on tidy data](https://tidyr.tidyverse.org/articles/tidy-data.html)

[More on ggplot2](https://ggplot2.tidyverse.org/)

[Key principes of effective data visualization](https://www.sciencedirect.com/science/article/pii/S2666389920301896)
